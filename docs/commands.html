<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>tccp - commands</title>
<link rel="stylesheet" href="style.css">
<link rel="icon" href="logo.png" type="image/png">
</head>
<body>
<h1>commands</h1>
<nav>
<a href="index.html">home</a>
<a href="install.html">install</a>
<a href="usage.html">usage</a>
<a href="commands.html">commands</a>
<a href="settings.html">settings</a>
<a href="architecture.html">how it works</a>
<a href="contact.html">contact</a>
</nav>

<p>
tccp has two kinds of commands. A few run from your regular shell (for
setup stuff), and the rest run inside the tccp REPL once you're connected.
</p>

<h2>shell commands</h2>
<p>These run from your terminal, before you connect:</p>

<h3><code>tccp setup</code></h3>
<p>Save your Tufts credentials (username and password). You only need to do
this once, but you can run it again if your password changes.</p>
<pre>$ tccp setup</pre>

<h3><code>tccp creds</code></h3>
<p>Same as <code>tccp setup</code> &mdash; update your stored credentials.
Handy if your password changed or you need to switch accounts.</p>
<pre>$ tccp creds</pre>

<h3><code>tccp register [path]</code></h3>
<p>Set up a project. This walks you through creating a <code>tccp.yaml</code>
that tells tccp how to run your code.</p>
<pre>
$ cd my-project
$ tccp register
</pre>

<h3><code>tccp manual [topic]</code></h3>
<p>Read the built-in guide for your project type.</p>
<pre>$ tccp manual python-pytorch</pre>

<hr>

<h2>REPL commands</h2>
<p>Once you're connected (<code>tccp</code> or <code>tccp connect</code>),
these are available at the prompt:</p>

<h3>jobs</h3>

<table>
<tr><th>command</th><th>what it does</th></tr>
<tr><td class="cmd">run &lt;job&gt; [args]</td><td>Start a job. Allocates a node, syncs your code, and launches it. You can pass extra args that get appended to whatever's in the config.</td></tr>
<tr><td class="cmd">view [job]</td><td>Attach to a running job's output. If the job already finished, opens it in a scrollable viewer.</td></tr>
<tr><td class="cmd">restart [job]</td><td>Cancel the current run and start a fresh one.</td></tr>
<tr><td class="cmd">jobs</td><td>Show all your tracked jobs &mdash; status, which node they're on, how long they've been running.</td></tr>
<tr><td class="cmd">cancel &lt;job&gt;</td><td>Stop a job. Works whether it's still initializing or already running.</td></tr>
<tr><td class="cmd">return [job]</td><td>Pull the output files from a finished job back to your laptop.</td></tr>
</table>

<pre>
tccp> run train --lr 0.001 --epochs 50
tccp> view train
tccp> jobs
tccp> cancel train
</pre>

<h3>looking at output</h3>

<table>
<tr><th>command</th><th>what it does</th></tr>
<tr><td class="cmd">logs [job]</td><td>Dump the job's output to your terminal.</td></tr>
<tr><td class="cmd">tail [job] [n]</td><td>Show the last N lines of output (30 by default).</td></tr>
<tr><td class="cmd">output [job]</td><td>Open the full output in vim.</td></tr>
<tr><td class="cmd">initlogs [job]</td><td>Show the initialization logs &mdash; useful for debugging allocation or sync issues.</td></tr>
</table>

<pre>
tccp> tail train 100
tccp> initlogs train
</pre>

<h3>getting on the node</h3>

<table>
<tr><th>command</th><th>what it does</th></tr>
<tr><td class="cmd">ssh [job]</td><td>SSH to a job's compute node. With no argument, uses the default job.</td></tr>
<tr><td class="cmd">clusterssh</td><td>SSH to the login node for running SLURM commands or poking around the cluster.</td></tr>
<tr><td class="cmd">open [job]</td><td>Drop into a bash shell inside the Singularity container, with your code, venv, and env vars all set up.</td></tr>
<tr><td class="cmd">exec &lt;cmd&gt;</td><td>Run a one-off command on the login node and print the result.</td></tr>
</table>

<pre>
tccp> open train
train@c0001> python -c "import torch; print(torch.cuda.is_available())"
True
</pre>

<h3>info</h3>

<table>
<tr><th>command</th><th>what it does</th></tr>
<tr><td class="cmd">status</td><td>Check your connection and project state.</td></tr>
<tr><td class="cmd">info [job]</td><td>Detailed breakdown: SLURM ID, node, timestamps, exit code.</td></tr>
<tr><td class="cmd">config</td><td>Print your current project config.</td></tr>
<tr><td class="cmd">allocs</td><td>List your SLURM allocations and what's using them.</td></tr>
<tr><td class="cmd">gpus</td><td>See what GPUs are available across partitions.</td></tr>
</table>

<h3>housekeeping</h3>

<table>
<tr><th>command</th><th>what it does</th></tr>
<tr><td class="cmd">dealloc [id|all]</td><td>Let go of SLURM allocations you're not using. They burn SUs while idle.</td></tr>
<tr><td class="cmd">clean</td><td>Delete old output directories, keeping only the latest run per job.</td></tr>
<tr><td class="cmd">refresh</td><td>Reload your tccp.yaml without disconnecting.</td></tr>
</table>

</body>
</html>
