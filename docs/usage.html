<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>tccp - usage</title>
<link rel="stylesheet" href="style.css">
<link rel="icon" href="logo.png" type="image/png">
</head>
<body>
<h1>usage</h1>
<nav>
<a href="index.html">home</a>
<a href="install.html">install</a>
<a href="architecture.html">how it works</a>
<a href="commands.html">commands</a>
<a href="usage.html">usage</a>
<a href="contact.html">contact</a>
</nav>

<h2>getting on the network</h2>
<p class="note">
You need to be on the Tufts network to use tccp.
</p>
<ul>
<li><b>Cisco AnyConnect VPN</b> (recommended) &mdash; connect once and you're good.</li>
<li><b>Tufts campus WiFi</b> &mdash; works fine, but requires MFA each time you start tccp.</li>
</ul>

<h2>first time</h2>
<pre>
# save your credentials (username, password, email)
$ tccp setup

# go to your project and register it
$ cd my-ml-project
$ tccp register

# connect
$ tccp
</pre>
<p>
<code>tccp setup</code> and <code>tccp register</code> are one-time things.
After that, you just run <code>tccp</code> from your project directory to
connect and start working.
</p>

<h2>tccp.yaml</h2>
<p>This lives in your project root. Here's a simple one:</p>
<pre>
type: python-pytorch
gpu: t4
env: .env
output: output
cache: cache

jobs:
  train:
    script: train.py
</pre>
<p>You can add more options per job. Everything here is optional &mdash;
tccp has reasonable defaults for all of it:</p>
<pre>
jobs:
  train:
    script: train.py
    time: "4:00:00"       # walltime limit (default: 1:00:00)
    args: "--lr 0.001"    # default arguments
    ports: [6006]         # auto-forwarded (tensorboard, jupyter, etc.)
    memory: "16G"         # per node
    gpu: a100             # override the project-level GPU
    gpu_count: 2          # more than one GPU
  eval:
    script: eval.py
    time: "0:30:00"
</pre>

<h2>secrets</h2>
<p>
Put API keys and tokens in a <code>.env</code> file in your project root.
tccp syncs it to the compute node automatically, and it's never tracked
by git.
</p>
<pre>
HF_TOKEN=hf_xxxxxxxxxxxxx
WANDB_API_KEY=xxxxxxxxxx
</pre>
<p>Use <code>os.environ["HF_TOKEN"]</code> or <code>python-dotenv</code>
to read them in your code.</p>

<h2>running jobs</h2>
<pre>
tccp> run train
</pre>
<p>Behind the scenes, tccp:</p>
<ol>
<li>Finds an idle allocation or requests a new one from SLURM</li>
<li>Makes sure the container, venv, and dtach are all set up</li>
<li>Syncs your code to the compute node (only files that changed)</li>
<li>Launches the job and attaches you to the output</li>
</ol>
<p>This all happens in the background. You can keep typing commands while
it sets up, and you'll get notified as each step finishes.</p>

<pre>
# pass extra arguments to your script
tccp> run train --epochs 100 --batch-size 64

# run multiple jobs at once
tccp> run train
tccp> run eval
</pre>

<h2>interactivity</h2>

<h3>attach and detach</h3>
<p>
<code>view</code> connects you to a running job's output stream. Hit
<code>Ctrl+C</code> to detach &mdash; the job keeps running in the background.
Come back anytime with <code>view</code> again.
</p>
<pre>
tccp> view train
... watching output ...
(Ctrl+C)
tccp> view train         # right back where you were
</pre>

<h3>scrolling through output</h3>
<p>
While you're viewing a job, <code>Up/Down</code> and <code>PgUp/PgDn</code>
let you scroll through everything the job has printed so far.
<code>q</code> exits scroll mode, <code>f</code> jumps back to following
live output.
</p>

<h3>getting a shell</h3>
<p>
<code>open</code> puts you inside a bash shell on the compute node, running
inside the same Singularity container as your job. Your code is there, your
venv is active, all your env vars are set.
</p>
<pre>
tccp> open train
train@c0001> python -c "import torch; print(torch.cuda.is_available())"
True
train@c0001> exit
</pre>

<h2>jobs survive disconnects</h2>
<p>
If your laptop sleeps, your WiFi drops, or you just close the terminal,
your jobs keep running on the cluster. Next time you run <code>tccp</code>,
everything's still there.
</p>
<pre>
$ tccp
tccp> jobs               # all your jobs, still going
tccp> view train         # reattach like nothing happened
</pre>
<p>
When a job finishes, tccp downloads the output for you automatically. If
you happen to be disconnected when it completes, it'll download next time
you connect.
</p>

<h2>allocation reuse</h2>
<p>
tccp doesn't throw away SLURM allocations when a job finishes. If you run
another job that needs the same resources, it reuses the existing node
instead of waiting in the queue again.
</p>
<pre>
tccp> allocs             # see what you have
tccp> dealloc all        # release everything when you're done for the day
</pre>
<p class="note">
Heads up: idle allocations still burn SUs. Don't forget to release them
when you're done working.
</p>

<h2>port forwarding</h2>
<p>
If your job runs a web server (TensorBoard, Jupyter, etc.), declare the
ports in your config and they get forwarded to localhost automatically.
</p>
<pre>
jobs:
  train:
    script: train.py
    ports: [6006]
</pre>
<p>
Then just open <code>http://localhost:6006</code> in your browser. No
SSH tunnel setup needed.
</p>

<h2>storage and caching</h2>
<p>
You don't need to think about this much, but it's good to know:
</p>
<ul>
<li>Your home directory has a small quota. tccp keeps only persistent
things there (container images, venvs, output files) and manages them
with LRU cleanup.</li>
<li>All runtime caches (pip, torch, HuggingFace) are redirected to
compute node /tmp, so they never eat into your quota.</li>
<li>Container pulls happen on the compute node where /tmp is large,
not on the DTN or your home dir. The final .sif is stored on NFS
so it's only pulled once.</li>
<li><code>requirements.txt</code> is installed automatically before each
run. tccp caches by hash, so if nothing changed it's a no-op.</li>
<li>The <code>cache</code> directory persists across jobs on the same
node &mdash; good for datasets or checkpoints you don't want to
re-download every run.</li>
<li>The <code>output</code> directory lives on NFS and gets synced back
to your laptop after each job finishes.</li>
</ul>

<h2>good to know</h2>
<ul>
<li>Your <code>.gitignore</code> (or <code>.tccpignore</code>) controls which files get synced. Keep large data files out.</li>
<li>If /tmp gets wiped (node reboot), tccp detects it and does a fresh sync automatically.</li>
<li>Changed your tccp.yaml? Type <code>refresh</code> to pick up the changes without reconnecting.</li>
</ul>

</body>
</html>
