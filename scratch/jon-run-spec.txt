I want this to be the template for running a python based job.

From the tccp terminal, say

run "job"

In the yaml, there will be a "jobs" list, and each job associated with a name. This name will be associated with "rodata" (a list of dir refs). when syncing, this is what i want

I want the process to set up a directory system in the projects scratch space. In this scratch space, there will by /tmp/<username>/<project>/<job_id>. this is the should be the program's current working directory. for the program. I want to make it seem like the program is running from here. Note that the program will be and can be referencing other files, (maybe from the other drive available to it, but this should be the on disk drive) . Let all the code and files that are not gitignored by copied into this place (with the exception of the directory "rodata" if it exists locally, since this might cause conflicts. There should also be additional directory /tmp/<username>/<project>/<job_id>/rodata/<dir_label>, these dir labels are going to be defined in the "rodata" top level section of the yaml where each element of it is a label corresponding to a absolute or relative (with respect to the yaml file) path to a directory. This is copied at startup. Another exception is a directory called output directory. /tmp/<username>/<project>/<job_id>/output will be a directory that is eventually synced back to the user on program termination. The idea is that the program expects to write some files (maybe output files like inference results or what) but it is actually symlinked to /cluster/home/<username>/tccp/<project>/<job-id>/output. Once the slurm job ends, it should BE AUTO DETECTED, success or failer, and if hte command prompt is still running it should take what is writteh nin the /cluster/home/<username>/tccp/<project>/<job-id> and bring it back to the LOCAL dir tccp/output/<job-id>. Notes, that sometimes if something crashes or connection doesnt work, the local and remove on /cluster might go out of sync, so this should be remedied by "return" command. think about this machanism. 

<job_id> should be YYYY-MM-DDTHH-MM-SS-mmm__<job-name> 

Also, I want to discuss code execution environments, speciically that always running on remote machine that we want to use singularity containers. Each job should have a type, and its type determines its env. but is likely can be defined for all (global stuff for project can be "tccp" name so this would be "tccp: \n type" which by default is python.

But for the python logic, I want you to setup the environment using singularity. Look this up. I basically want to use the singularity container alongside a mounted /envs/ directory to be the actually complete environment of the project. This env directiory should be At /cluster/home/<username>/<project>/env/default. We might do more advanced functionality latter. but put the python's necessary <venv> in the /env/defaults and use it for the execution. Create the /env/default and venv environment. This is where deps should be installed too. There should be health checks for this env, and the directory structure everytime i run something incrementally, and if they dont pass, it should be intiated.


DONE OUTSIDE CONTAINER INIT
Does directory structure enven proper exist? if not, fix it
Does env/default exist, make sure it exists.

INSIDE PYTHON INIT
if python project, is there a venv there, if not instantiate it. 
If the python enviromnet exists, use the local venv to install pip install -r requirements.txt. if they are synced nothing should happen, if they are not it should sync.

The dependencies should outlive the job, and be resused by multiple jobs in the same project. 

ALSO IMPORTANT.  Use Singularity for Python runtime, and make the project environment persistent at /cluster/home/<username>/<project>/env/default. Create /env/default/venv with python -m venv via Singularity, install all deps into that venv, and always execute with singularity exec <image.sif> /cluster/home/<username>/<project>/env/default/venv/bin/python ... (and pip via the same venv path). Keep image/cache project-scoped (e.g., <project>/container-cache/{images,cache,tmp}) and treat env/default as the canonical mutable env for this project.
